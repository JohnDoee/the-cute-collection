import concurrent.futures
import dataclasses
import json
import math
import re
import shlex
import threading
import time
import traceback
from collections import namedtuple
from pathlib import Path

import click
import cv2
import jinja2
import lxml.html
import matplotlib
import matplotlib.pyplot as plt
import numpy as np
import pysubs2
import pytesseract
from pyxdameraulevenshtein import normalized_damerau_levenshtein_distance
from scipy.spatial.distance import cdist, cosine
from skimage.metrics import structural_similarity
from textblob import TextBlob
from tqdm import tqdm, trange

BASE_ASS = r"""[Script Info]
; Script generated by Aegisub 3.2.2
; http://www.aegisub.org/
Title: CowOCR
ScriptType: v4.00+
WrapStyle: 0
ScaledBorderAndShadow: yes
PlayResX: 640
PlayResY: 480

[V4+ Styles]
Format: Name, Fontname, Fontsize, PrimaryColour, SecondaryColour, OutlineColour, BackColour, Bold, Italic, Underline, StrikeOut, ScaleX, ScaleY, Spacing, Angle, BorderStyle, Outline, Shadow, Alignment, MarginL, MarginR, MarginV, Encoding
Style: Default,Open Sans Semibold,26.0,&H00FFFFFF,&H000000FF,&H00020713,&H00000000,-1,0,0,0,100.0,100.0,0.0,0.0,1,1.7,0.0,2,0,0,28,1
Style: Sign,Open Sans Semibold,26.0,&H00FFFFFF,&H000000FF,&H00020713,&H00000000,-1,0,0,0,100.0,100.0,0.0,0.0,1,1.7,0.0,2,0,0,28,1
Style: Note,Open Sans Semibold,20.0,&H00FFFFFF,&H000000FF,&H00020713,&H00000000,-1,0,0,0,100.0,100.0,0.0,0.0,1,1.7,0.0,8,0,0,28,1

[Events]
Format: Layer, Start, End, Style, Name, MarginL, MarginR, MarginV, Effect, Text
"""

HTML_BASE = r"""<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-+0n0xVW2eSR5OomGNYDnhzAbDsOXxcvSN1TPprVMTNDbiYZCxYbOOl7+AMvyTG2x" crossorigin="anonymous">

    <title>{{ title }}</title>
  </head>
  <body>
    <nav class="navbar navbar-expand-lg navbar-light bg-light">
    <div class="container-fluid">
        <a class="navbar-brand" href="index.html">OCR Report</a>
        <!---<ul class="navbar-nav me-auto mb-2 mb-lg-0">
            <li class="nav-item">
                <a class="nav-link" href="subtitles.html">Subtitles</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="subtitle-signs.html">Subtitle Signs</a>
            </li>
            <li class="nav-item">
                <a class="nav-link" href="missing-regions.html">Missing regions</a>
            </li>
        </ul>---->
    </div>
    </nav>
    %(body)s
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.0.1/dist/js/bootstrap.bundle.min.js" integrity="sha384-gtEjrD/SeCtmISkJkNUaaKMoLD0//ElJ19smozuHV6z3Iehds+3Ulb9Bn9Plx0x4" crossorigin="anonymous"></script>
  </body>
</html>"""

HTML_INDEX = HTML_BASE % {
    "body": r"""
    <h1>OCR Report for {{ video_filename }}</h1>
    {% for sr in subtitle_regions %}
    <div class="mb-3">
    <h4>Subtitle Region: {{sr.name}}</h4>
        {% if sr.scan_mode == 'bottom_center' %}
            <a href="{{ sr.name }}-subtitles.html">Subtitle lines<span style="display: none;"> ({{ subtitle_region_data[sr.name]['subtitle_lines']|length }})</span></a>
            <a href="{{ sr.name }}-missing-regions.html">Missing regions<span style="display: none;"> ({{ subtitle_region_data[sr.name]['missing_regions']|length }})</span></a>
        {% endif %}
        {% if sr.scan_mode == 'search_slice' %}
            <a href="{{ sr.name }}-subtitle-signs.html">Subtitle signs<span style="display: none;"> ({{ subtitle_region_data[sr.name]['subtitle_signs']|length }})</span></a>
            <a href="{{ sr.name }}-missing-regions.html">Missing regions<span style="display: none;"> ({{ subtitle_region_data[sr.name]['missing_regions']|length }})</span></a>
        {% endif %}
    </div>
    {% endfor %}
"""
}

HTML_SUBTITLE_LINES = HTML_BASE % {
    "body": r"""
    <h1>Subtitle lines for: {{ sr.name }}</h1>
    <table class="table table-striped">
        <thead>
            <tr>
                <th scope="col">#</th>
                <th scope="col">Timestamps</th>
                <th scope="col">Text</th>
                <th scope="col">Frames</th>
            </tr>
        </thead>
        <tbody>
        {% for subtitle_line in subtitle_lines %}
            <tr>
                <td>{{loop.index}}</td>
                <td>
                    {{ subtitle_line['from_frame_no']|totimestamp }}<br>{{ subtitle_line['to_frame_no']|totimestamp }}<br>
                    {{ subtitle_line['from_frame_no'] }}<br>{{ subtitle_line['to_frame_no'] }}<br>{{ subtitle_line['initial_frame_no'] }}
                </td>
                <td>
                    {% autoescape false %}{{ subtitle_line['subtitle_text'] | replace('\n', '<br>') }}{% endautoescape %}<br>
                </td>
                <td>
                    <img class="mb-1" src="{{ get_frame(subtitle_line['from_frame_no'] - 1, None, sr) }}"><br>
                    <img class="mb-1" src="{{ get_frame(subtitle_line['from_frame_no'], None, sr) }}"><br>
                    <img class="mb-1" src="{{ get_frame(subtitle_line['to_frame_no'], None, sr) }}"><br>
                    <img src="{{ get_frame(subtitle_line['to_frame_no'] + 1, None, sr) }}">
                </td>
            </tr>
        {% endfor %}
        </tbody>
    </table>
"""
}

HTML_MISSING_REGIONS = HTML_BASE % {
    "body": r"""
    <h1>Missed regions for: {{ sr.name }}</h1>
    <table class="table table-striped">
        <thead>
            <tr>
                <th scope="col">#</th>
                <th scope="col">Timestamps</th>
                <th scope="col">Frame</th>
            </tr>
        </thead>
        <tbody>
        {% for missing_region in missing_regions %}
            <tr>
                <td>{{loop.index}}</td>
                <td>{{ missing_region['frame_no']|totimestamp }}<br>{{ missing_region['frame_no'] }}</td>
                <td><img src="{{ get_frame(missing_region['frame_no'], None) }}"></td>
            </tr>
        {% endfor %}
        </tbody>
    </table>

    <h1 class="mt-3">Short subtitle signs</h1>
    <table class="table table-striped">
        <thead>
            <tr>
                <th scope="col">#</th>
                <th scope="col">Timestamps</th>
                <th scope="col">Frame</th>
            </tr>
        </thead>
        <tbody>
        {% for subtitle_sign in short_subtitle_signs %}
            <tr>
                <td>{{loop.index}}</td>
                <td>
                    {{ subtitle_sign['from_frame_no']|totimestamp }}<br>{{ subtitle_sign['to_frame_no']|totimestamp }}<br>
                    {{ subtitle_sign['from_frame_no'] }}<br>{{ subtitle_sign['to_frame_no'] }}<br>{{ subtitle_sign['initial_frame_no'] }}
                </td>
                <td><img src="{{ get_frame(subtitle_sign['from_frame_no'], None) }}"></td>
            </tr>
        {% endfor %}
        </tbody>
    </table>
"""
}

HTML_SUBTITLE_SIGNS = HTML_BASE % {
    "body": r"""
    <h1>Subtitle signs for: {{ sr.name }}</h1>
    <table class="table table-striped">
        <thead>
            <tr>
                <th scope="col">#</th>
                <th scope="col">Timestamps</th>
                <th scope="col">Frame</th>
            </tr>
        </thead>
        <tbody>
        {% for subtitle_sign in subtitle_signs %}
            <tr>
                <td>{{loop.index}}</td>
                <td>
                    {{ subtitle_sign['from_frame_no']|totimestamp }}<br>{{ subtitle_sign['to_frame_no']|totimestamp }}<br>
                    {{ subtitle_sign['from_frame_no'] }}<br>{{ subtitle_sign['to_frame_no'] }}<br>{{ subtitle_sign['initial_frame_no'] }}
                </td>
                <td>{% autoescape false %}{{ subtitle_sign['subtitle_text'] | replace('\n', '<br>') }}{% endautoescape %}</td>
                <td><img src="{{ get_frame(subtitle_sign['from_frame_no'] - 1, subtitle_sign['position']) }}"></td>
                <td><img src="{{ get_frame(subtitle_sign['from_frame_no'], subtitle_sign['position']) }}"></td>
                <td><img src="{{ get_frame(subtitle_sign['to_frame_no'], subtitle_sign['position']) }}"></td>
                <td><img src="{{ get_frame(subtitle_sign['to_frame_no'] + 1, subtitle_sign['position']) }}"></td>
            </tr>
        {% endfor %}
        </tbody>
    </table>
"""
}


@dataclasses.dataclass
class SubtitleRegion:
    class ScanMode:
        BOTTOM_CENTER = "bottom_center"
        SEARCH_SLICE = "search_slice"

    class InvertMode:
        NO_INVERT = "no_invert"
        INVERT_ONLY = "invert_only"
        BOTH_INVERT = "both_invert"

    name: str
    scan_mode: ScanMode
    y: int
    h: int
    x: int
    w: int
    margin: int
    area_min: int
    area_max: int
    area_min_density: float
    max_w: int
    max_h: int
    min_stroke_width: int
    max_stroke_width: int
    border_size: int
    max_text_diff: int
    max_border_diff: int
    percent_good_border: float
    edge_threshold: int
    threshold_mode: str
    threshold_value: int
    ass_style_name: str
    invert_mode: InvertMode


default_subtitle_regions = [
    SubtitleRegion(
        name="bottom",
        scan_mode=SubtitleRegion.ScanMode.BOTTOM_CENTER,
        y=376,
        h=100,
        x=0,
        w=640,
        margin=4,
        area_min=6,
        area_max=1000,
        area_min_density=0.2,
        max_w=160,
        max_h=40,
        min_stroke_width=2,
        max_stroke_width=7,
        border_size=2,
        max_text_diff=60,
        max_border_diff=60,
        percent_good_border=25,
        edge_threshold=8,
        threshold_mode="static",
        threshold_value=200,
        ass_style_name="Default",
        invert_mode=SubtitleRegion.InvertMode.NO_INVERT,
    ),
    SubtitleRegion(
        name="region-scan",
        scan_mode=SubtitleRegion.ScanMode.SEARCH_SLICE,
        y=0,
        h=380,
        x=0,
        w=640,
        margin=0,
        area_min=6,
        area_max=1000,
        area_min_density=0.2,
        max_w=160,
        max_h=50,
        min_stroke_width=2,
        max_stroke_width=7,
        border_size=2,
        max_text_diff=60,
        max_border_diff=60,
        percent_good_border=25,
        edge_threshold=3,
        threshold_mode="static",
        threshold_value=200,
        ass_style_name="Sign",
        invert_mode=SubtitleRegion.InvertMode.NO_INVERT,
    ),
]


def crop(image):
    y_nonzero, x_nonzero, _ = np.nonzero(image)
    if len(y_nonzero) == 0:
        return None
    crop_space = 4
    y, x, = max(
        np.min(y_nonzero) - crop_space, 0
    ), max(np.min(x_nonzero) - crop_space, 0)
    h, w = np.max(y_nonzero) + crop_space - y, np.max(x_nonzero) + crop_space - x
    return image[y : y + h, x : x + w], (y, h, x, w)


def ocr_region(frame_region, tesseract_data_path):
    custom_oem_psm_config = f"--psm 6 --tessdata-dir {shlex.quote(tesseract_data_path)}"  # TODO, get datapath
    return pytesseract.image_to_pdf_or_hocr(
        frame_region, extension="hocr", config=custom_oem_psm_config, lang="eng"
    )


def ocr_region_from_mask(
    frame, mask, tesseract_data_path, gather_images=None, inverted=False
):
    mask = cv2.dilate(
        mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=1
    )
    choppable_frame = frame & cv2.cvtColor(mask, cv2.COLOR_GRAY2RGB)
    if inverted:
        choppable_frame[mask == 0] = [255, 255, 255]
        choppable_frame = cv2.bitwise_not(choppable_frame)

    crop_result = crop(choppable_frame)
    if crop_result is None:
        return None, None, None
    image, crop_region = crop_result
    image = cv2.bitwise_not(image)
    image = cv2.resize(image, (image.shape[1] * 2, image.shape[0] * 2))
    image = cv2.blur(image, (2, 2))
    if gather_images is not None:
        gather_images.append(("Image for OCR", image))
    return image, crop_region, ocr_region(image, tesseract_data_path)


def frame_has_mask(src_frame, dst_frame, mask, min_percent=0.1, max_offset=12):
    orig_mask = mask.copy()
    orig_mask = cv2.cvtColor(orig_mask, cv2.COLOR_GRAY2BGR)
    mask = mask.astype("bool")
    c = np.abs(src_frame[mask].flatten() - dst_frame[mask].flatten()).astype("uint8")
    c += max_offset
    return len(c[c > (max_offset * 2)]) < int(min_percent * len(c))


def find_frames_with_mask(
    video,
    initial_frame_no,
    src_frame,
    mask,
    shape,
    region,
    max_frames_backward=18,
    max_frames_forward=600,
    max_frames=None,
):
    frames_with_mask = []
    for r in [range(0, -max_frames_backward - 1, -1), range(1, max_frames_forward)]:
        for i in r:
            frame_no = initial_frame_no + i
            if frame_no >= max_frames:
                break
            if frame_no > 0:
                dst_frame = get_frame(video, frame_no, shape, region=region)
            else:
                dst_frame = None
            if dst_frame is None or not frame_has_mask(src_frame, dst_frame, mask):
                break
            frames_with_mask.append(frame_no)
    if not frames_with_mask:
        return None, None

    return min(frames_with_mask), max(frames_with_mask)


def remove_smaller_cc(mask):
    dilated_mask = cv2.dilate(
        mask.copy(), cv2.getStructuringElement(cv2.MORPH_RECT, (3, 5)), iterations=6
    )
    numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(
        dilated_mask, connectivity=4
    )
    sizes = stats[:, -1]
    i = np.argmax(sizes[1:]) + 1
    labels[labels != i] = 0
    labels[labels == i] = 255
    return mask & labels.astype("uint8"), sizes[i]


def estimate_line_width(labels):
    labels = labels.copy()
    labels = np.repeat(labels, 2, axis=0)
    labels = np.repeat(labels, 2, axis=1)
    mask = labels.copy()
    mask[mask > 0] = 1
    mask = mask.astype("uint8")
    label_line_width = {}
    i = 0
    current_labels = set(np.unique(labels))

    label_count = {}
    for label, count in zip(*np.unique(labels, return_counts=True)):
        label_count[label] = [count]
    while True:
        mask = cv2.erode(
            mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=1
        )
        i += 1
        new_labels = set(np.unique(labels * mask))
        label_line_width[i] = current_labels - new_labels
        if len(new_labels) <= 1:
            break
        current_labels = new_labels
        for label, count in zip(*np.unique(labels * mask, return_counts=True)):
            label_count[label].append(count)
    return label_line_width


def find_potential_text_block_areas(orig_labels, iterations=7):
    mask = orig_labels.copy()
    mask[mask > 0] = 255
    mask = mask.astype("uint8")
    mask = cv2.dilate(
        mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=iterations
    )
    numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(
        mask, connectivity=4
    )
    shape = labels.shape
    mid = shape[1] // 2
    for i, (x, y, w, h, area) in enumerate(stats):
        if x < mid and x + w > mid and h >= 20:
            labels[labels == i] = 0
    region_space = 16
    source_region = np.zeros(mask.shape, dtype="bool")
    source_region[0 : mask.shape[0], mid - region_space : mid + region_space] = True
    return np.unique(orig_labels[labels.astype("bool")]), source_region


def do_threshold(sr, frame):
    if sr.threshold_mode == "adaptive":
        thresh = cv2.adaptiveThreshold(
            cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
            255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY,
            sr.threshold_value,
            2,
        )
    elif sr.threshold_mode == "static":
        thresh = cv2.threshold(
            cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY),
            sr.threshold_value,
            255,
            cv2.THRESH_BINARY,
        )[1]
    return thresh


def extract_text_subregion(
    frame, sr, tesseract_data_path, gather_images=None, inverted=False
):
    thresh = do_threshold(sr, frame)
    if inverted:
        thresh = cv2.bitwise_not(thresh)
    numLabels, labels = cv2.connectedComponents(thresh, connectivity=4)
    for label, count in zip(*np.unique(labels, return_counts=True)):
        if count <= sr.area_max:
            labels[labels == label] = 0
    labels[labels > 0] = 255
    thresh = thresh - labels.astype("uint8")

    numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(
        thresh, connectivity=4
    )
    subtitle_region_labels = labels.copy()
    if gather_images is not None:
        gather_images.append(("Thresh", thresh))

    label_stats = {}
    for i, (x, y, w, h, area) in enumerate(stats):
        if i == 0:
            continue
        if (
            sr.max_h >= h
            and sr.max_w >= w
            and sr.area_min <= area
            and sr.area_max >= area
            and sr.margin <= y
            and frame.shape[0] - sr.margin >= y + h  # TODO: margin remove
            and sr.margin <= x
            and frame.shape[1] - sr.margin >= x + w
            and sr.area_min_density < (area / (h * w))
        ):
            label_stats[i] = (x, y, w, h, area)
        else:
            labels[labels == i] = 0

    dust_labels, starting_area = find_potential_text_block_areas(labels)
    for i in dust_labels:
        if i == 0:
            continue
        labels[labels == i] = 0
        del label_stats[i]

    estimated_line_width = estimate_line_width(labels)
    correct_stroke_width = [
        vv
        for v in [
            v
            for (k, v) in estimated_line_width.items()
            if k <= sr.max_stroke_width and k >= sr.min_stroke_width
        ]
        for vv in v
    ]
    labels[np.isin(labels, correct_stroke_width, invert=True)] = 0

    if gather_images is not None:
        gather_images.append(("final mask", labels))

    mask = labels.astype("bool")

    pixels = frame[mask]
    if len(pixels) == 1:
        return None, None
    pixels = np.float32(pixels)
    n_colors = min(20, len(pixels))
    if n_colors == 0:
        return None, None
    criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1)
    flags = cv2.KMEANS_PP_CENTERS
    _, color_labels, palette = cv2.kmeans(pixels, n_colors, None, criteria, 10, flags)
    _, counts = np.unique(color_labels, return_counts=True)

    starting_area_labels = np.full(labels.shape, 255, dtype="uint8")
    starting_area_labels[mask] = color_labels.flatten()

    starting_labels, starting_labels_counts = np.unique(
        starting_area_labels[starting_area], return_counts=True
    )
    starting_labels_label_enum = sorted(
        [(l, c) for (l, c) in zip(starting_labels, starting_labels_counts) if l != 255],
        key=lambda x: x[1],
        reverse=True,
    )
    starting_labels = [x[0] for x in starting_labels_label_enum]

    consumed_labels = set()
    label_enum = sorted(enumerate(counts), key=lambda l: l[1], reverse=True)
    region_candidates = []
    for label, _ in label_enum:
        if label not in starting_labels:
            continue
        if label in consumed_labels:
            continue
        dominant_labels = [label]
        dominant = palette[label]
        for i, color in enumerate(palette):
            if i == label:
                continue
            if np.linalg.norm(dominant - color) < sr.max_text_diff:
                dominant_labels.append(i)

        consumed_labels |= set(dominant_labels)
        labels_inner = labels.copy()
        dominant_color_labels = color_labels.copy()
        dominant_color_labels[
            np.isin(dominant_color_labels, list(dominant_labels), invert=True)
        ] = 0
        dominant_color_labels[dominant_color_labels >= 1] = 1

        labels_subset = np.unique(
            labels[mask][dominant_color_labels.astype("bool").flatten()]
        )
        labels_subset = labels_subset[labels_subset != 0]

        labels_inner[np.isin(labels_inner, labels_subset, invert=True)] = 0
        labels_inner[labels_inner > 0] = 255
        labels_inner = labels_inner.astype("uint8")

        labels_border = (
            cv2.dilate(
                labels_inner,
                cv2.getStructuringElement(
                    cv2.MORPH_RECT, (sr.border_size * 2 + 1, sr.border_size * 2 + 1)
                ),
                iterations=1,
            )
            - labels_inner
        )
        border_pixels = frame[labels_border.astype("bool")]
        border_pixels = np.float32(border_pixels)
        border_n_colors = min(20, len(border_pixels))

        if border_n_colors == 0:
            continue

        border_criteria = (cv2.TERM_CRITERIA_EPS + cv2.TERM_CRITERIA_MAX_ITER, 200, 0.1)
        border_flags = cv2.KMEANS_PP_CENTERS
        _, border_color_labels, border_palette = cv2.kmeans(
            border_pixels, border_n_colors, None, border_criteria, 10, border_flags
        )
        _, border_counts = np.unique(border_color_labels, return_counts=True)

        starting_area_border_labels = np.full(labels.shape, 255, dtype="uint8")
        starting_area_border_labels[
            labels_border.astype("bool")
        ] = border_color_labels.flatten()

        consumed_border_labels = set()
        border_label_enum = sorted(
            enumerate(border_counts), key=lambda l: l[1], reverse=True
        )
        for label, _ in border_label_enum:
            if label in consumed_border_labels:
                continue
            consumed_border_labels.add(label)
            dominant_labels_border = [label]
            dominant = border_palette[label]
            for i, color in enumerate(border_palette):
                if i in consumed_border_labels:
                    continue
                if i == label:
                    continue
                if np.linalg.norm(dominant - color) < sr.max_border_diff:
                    # print(f"Merging {i=} {dominant=} and {color=}")
                    dominant_labels_border.append(i)
                    consumed_border_labels.add(i)
            break

        frame_border = np.zeros(frame.shape[:2], dtype="uint8")
        frame_border[labels_border.astype("bool")] = border_color_labels.flatten() + 1

        border_labels_subset = set(labels_subset)
        for i, (x, y, w, h, area) in enumerate(stats):
            if i not in border_labels_subset:
                continue
            mask_label = labels.copy()
            mask_label[mask_label != i] = 0
            mask_label[mask_label == i] = 255
            mask_label = mask_label.astype("uint8")
            mask_label_border = (
                cv2.dilate(
                    mask_label,
                    cv2.getStructuringElement(
                        cv2.MORPH_RECT, (sr.border_size * 2 + 1, sr.border_size * 2 + 1)
                    ),
                    iterations=1,
                )
                - mask_label
            )
            border_frame = frame_border & mask_label_border
            border_labels, border_counts = np.unique(border_frame, return_counts=True)
            border_labels = border_labels[1:] - 1
            border_counts = border_counts[1:]
            good, bad = 0, 0
            for border_label, border_count in zip(border_labels, border_counts):
                if border_label in dominant_labels_border:
                    good += border_count
                else:
                    bad += border_count
            if round((good / (good + bad)) * 100) < sr.percent_good_border:
                border_labels_subset.remove(i)

        border_labels_subset = np.array(list(border_labels_subset))
        result_mask = labels.copy()
        result_mask[np.isin(result_mask, border_labels_subset, invert=True)] = 0
        result_mask[result_mask > 0] = 255
        result_mask = result_mask.astype("uint8")
        # result_mask = cv2.dilate(result_mask, cv2.getStructuringElement(cv2.MORPH_RECT, (3, 3)), iterations=1)
        if gather_images is not None:
            gather_images.append(("final region label", result_mask))
        image, crop_region, hocr = ocr_region_from_mask(
            frame,
            result_mask,
            tesseract_data_path,
            gather_images=gather_images,
            inverted=inverted,
        )
        if hocr is not None and hocr_to_text(hocr):
            if gather_images is not None:
                print("Found HOCR")
                print(hocr.decode())
            region_candidates.append((result_mask, hocr))
    return region_candidates


def find_diff_regions(
    src_frame,
    dst_frame,
    gather_images=None,
    nfeatures=2000,
    min_kp_region=100,
    edge_threshold=14,
):
    orb = cv2.ORB_create(
        edgeThreshold=edge_threshold, patchSize=edge_threshold, nfeatures=nfeatures
    )
    src_kp = orb.detect(src_frame, None)
    if gather_images is not None:
        gather_images.append(
            (
                "Source keypoints",
                cv2.drawKeypoints(src_frame, src_kp, None, color=(0, 255, 0), flags=0),
            )
        )

    orb = cv2.ORB_create(
        edgeThreshold=edge_threshold, patchSize=edge_threshold, nfeatures=50000
    )
    dst_kp = orb.detect(dst_frame, None)
    if gather_images is not None:
        gather_images.append(
            (
                "Destination keypoints",
                cv2.drawKeypoints(dst_frame, dst_kp, None, color=(0, 255, 0), flags=0),
            )
        )

    kp_mask = np.zeros(src_frame.shape[:2], dtype="uint8")

    src_kp_vector = np.array([k.pt + (k.angle / 5,) for k in src_kp])
    dst_kp_vector = np.array([k.pt + (k.angle / 5,) for k in dst_kp])
    if len(src_kp_vector) == 0:
        return kp_mask

    if len(dst_kp_vector) == 0:
        good_kps = list(src_kp)
    else:
        C = cdist(dst_kp_vector, src_kp_vector, metric="euclidean")
        good_kps = []
        for k, cost in zip(src_kp, np.min(C, axis=0)):
            if cost > 6.0:
                good_kps.append(k)

    if gather_images is not None:
        kp_diff_src = cv2.drawKeypoints(
            src_frame, good_kps, None, color=(0, 255, 0), flags=0
        )
        gather_images.append(("KP Diff Source", kp_diff_src))

    for kp in good_kps:
        x, y = kp.pt
        kp_mask[int(y), int(x)] = 255

    mask = cv2.dilate(
        kp_mask, cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3)), iterations=6
    )
    numLabels, labels = cv2.connectedComponents(mask, connectivity=8)
    for i in range(1, numLabels):
        label_mask = labels.copy()
        label_mask[label_mask != i] = 0
        label_mask = kp_mask & label_mask.astype(bool)
        if len(label_mask[label_mask > 0]) < min_kp_region:
            labels[labels == i] = 0

    labels[labels > 0] = 255
    mask = labels.astype("uint8")
    if gather_images is not None:
        gather_images.append(("Mask", mask))

    return mask


def evaluate_if_inverted(sr, frame, mask):
    thresh = do_threshold(sr, frame)

    numLabels, labels = cv2.connectedComponents(thresh & mask, connectivity=4)
    numLabelsInverted, labelsInverted = cv2.connectedComponents(
        cv2.bitwise_not(thresh) & mask, connectivity=4
    )

    return numLabels < numLabelsInverted


def get_frame(video, frame_no, shape=None, region=None, skip_align=False):
    if frame_no < 0:
        raise Exception(f"Trying to get {frame_no}")

    video.set(cv2.CAP_PROP_POS_FRAMES, frame_no)
    ret, frame = video.read()

    if not ret:
        raise Exception(f"Failed to get frame {frame_no}")

    if skip_align:
        return frame

    if shape is not None:
        sy, sx, sz = shape
        fy, fx, fz = frame.shape
        frame_aspect = fx / fy

        if frame_aspect > sx / sy:
            new_fx = frame_aspect * sy
            slice_each_x = int((fx - new_fx) / 2)
            frame = frame[0:fy, slice_each_x : (fx - slice_each_x)]
        elif frame_aspect < sx / sy:
            new_fx = (sx / sy) * fy
            append_each_x = int((new_fx - fx) / 2)
            if append_each_x > 0:
                frame = cv2.copyMakeBorder(
                    frame,
                    0,
                    0,
                    append_each_x,
                    append_each_x,
                    cv2.BORDER_CONSTANT,
                    None,
                    (0, 0, 0),
                )

        if frame.shape > shape:
            frame = cv2.resize(frame, (sx, sy))

    if region is not None:
        frame = frame[region[0] : region[1], region[2] : region[3]]

    return frame


def hocr_to_text(hocr):
    def parse_hocr_title(title):
        result = {}
        for entry in title.split(";"):
            if not entry.strip():
                continue
            entry_key, entry_value = entry.strip().split(" ", 1)
            if entry_key in ["bbox", "x_ascenders", "x_wconf"]:
                result[entry_key] = [
                    int(float(x)) for x in entry_value.strip().split(" ")
                ]
            elif entry_key in ["x_size", "x_descenders", "baseline"]:
                result[entry_key] = [float(x) for x in entry_value.strip().split(" ")]
        return result

    tree = lxml.html.fromstring(hocr)
    lines, score, line_consumptions = [], [], []
    for line in tree.xpath(r"//span[@class='ocr_line']"):
        line_specs = parse_hocr_title(line.attrib["title"])
        line_consumption = []
        line_text = []
        for word in line.xpath(r"./span[@class='ocrx_word']"):
            specs = parse_hocr_title(word.attrib["title"])
            score.append(specs["x_wconf"])
            line_consumption.append(specs["bbox"][2] - specs["bbox"][0])
            line_text.append(word.text)
        lines.append(" ".join(line_text))
        line_consumptions.append(
            min(sum(line_consumption), line_specs["bbox"][2] - line_specs["bbox"][0])
        )
    if not lines:
        return 0, "", []
    return np.average(np.array(score)), "\n".join(lines), line_consumptions


def get_subtitle_mask(
    frame, sr, tesseract_data_path, gather_images=None, inverted=False
):
    subregion_candidates = extract_text_subregion(
        frame, sr, tesseract_data_path, gather_images=gather_images, inverted=inverted
    )
    if subregion_candidates and subregion_candidates[0] is not None:
        candidate_scores = []
        for result_mask, hocr in subregion_candidates:
            score, text, line_consumptions = hocr_to_text(hocr)
            if not text or not line_consumptions:
                continue
            percent_filled = sorted(line_consumptions, reverse=True)[0] / frame.shape[1]
            candidate_scores.append(
                (score * percent_filled, score, percent_filled, result_mask, text)
            )
            if score > 65 and percent_filled > 0.8:
                return result_mask, text
        if candidate_scores:
            return sorted(candidate_scores, reverse=True, key=lambda x: x[0])[0][3:]
    return None, None


def slice_mask_regions(
    frame,
    mask,
    sr,
    orig_region,
    tesseract_data_path,
    gather_images=None,
    inverted=False,
):
    subtitles = []
    numLabels, labels, stats, centroids = cv2.connectedComponentsWithStats(
        mask, connectivity=4
    )
    for i, (x, y, w, h, area) in enumerate(stats):
        if i == 0:
            continue
        region = (max(y - 4, 0), y + h + 4, max(x - 4, 4), x + w + 4)
        sliced_frame = frame[region[0] : region[1], region[2] : region[3]].copy()
        subtitle_mask, subtitle_text = get_subtitle_mask(
            sliced_frame,
            sr,
            tesseract_data_path,
            gather_images=gather_images,
            inverted=inverted,
        )
        if subtitle_text:
            subtitle_region = (
                region[0] + orig_region[0],
                region[0] + sliced_frame.shape[0] + orig_region[0],
                region[2] + orig_region[2],
                region[2] + sliced_frame.shape[1] + orig_region[2],
            )
            subtitles.append(
                (subtitle_mask, subtitle_text, sliced_frame, subtitle_region)
            )
        else:
            subtitles.append((subtitle_mask, None, sliced_frame, None))
    return subtitles


def loop_frames(
    src_video,
    dst_video,
    start_frame_no,
    end_frame_no,
    frame_diff,
    sr,
    tesseract_data_path,
    frame_steps=9,
    fix_broken_frame_alignment=False,
    edge_threshold=14,
    gather_images=None,
    max_frames=None,
):
    src_video.set(cv2.CAP_PROP_POS_FRAMES, 0)
    _, src_frame = src_video.read()

    shape = src_frame.shape
    region = (sr.y, sr.y + sr.h, sr.x, sr.x + sr.w)
    min_frame_no = 0

    if frame_diff < 0:
        start_frame_no = max(start_frame_no - frame_diff, 0)

    for frame_no in tqdm(
        range(start_frame_no, end_frame_no, frame_steps),
        desc=f"Region: {sr.name} Framerange: {start_frame_no}-{end_frame_no}",
    ):
        if frame_no < min_frame_no:
            continue
        if fix_broken_frame_alignment:
            last_frame_no = frame_no + 2
            if max_frames is not None:
                last_frame_no = min(last_frame_no, max_frames)
            frame_no_range = range(max(frame_no - 1, 0, -frame_diff), last_frame_no)
        else:
            frame_no_range = range(frame_no, frame_no + 1)

        frames = []

        for frame_no_actual in frame_no_range:
            src_frame = get_frame(src_video, frame_no_actual, shape, region=region)
            dst_frame = get_frame(
                dst_video, frame_no_actual + frame_diff, shape, region=region
            )

            frames.append(
                (
                    src_frame,
                    dst_frame,
                    frame_no_actual,
                    structural_similarity(src_frame, dst_frame, multichannel=True),
                )
            )

        src_frame, dst_frame, frame_no = sorted(
            frames, key=lambda x: x[3], reverse=True
        )[0][:3]
        if gather_images is not None:
            gather_images.append((f"Source frame {frame_no}", src_frame))
            gather_images.append((f"Destination frame {frame_no}", dst_frame))

        mask = find_diff_regions(
            src_frame,
            dst_frame,
            gather_images=gather_images,
            edge_threshold=edge_threshold,
        )
        values, counts = np.unique(mask, return_counts=True)

        if len(counts) > 1 and counts[1] > 150:
            inverted = evaluate_if_inverted(sr, src_frame, mask)
            found_to_frames = []
            subtitles = []
            if sr.scan_mode == SubtitleRegion.ScanMode.SEARCH_SLICE:
                subtitles += slice_mask_regions(
                    src_frame,
                    mask,
                    sr,
                    region,
                    tesseract_data_path,
                    gather_images=gather_images,
                    inverted=inverted,
                )
            else:
                subtitles.append(
                    get_subtitle_mask(
                        src_frame,
                        sr,
                        tesseract_data_path,
                        gather_images=gather_images,
                        inverted=inverted,
                    )
                    + (
                        src_frame,
                        region,
                    )
                )

            for (
                subtitle_mask,
                subtitle_text,
                subtitle_src_frame,
                subtitle_region,
            ) in subtitles:
                if gather_images is not None:
                    gather_images.append(
                        (f"Frame: {frame_no} - text: {subtitle_text}", subtitle_mask)
                    )

                if subtitle_text:
                    from_frame_no, to_frame_no = find_frames_with_mask(
                        src_video,
                        frame_no,
                        subtitle_src_frame,
                        subtitle_mask,
                        shape,
                        subtitle_region,
                        max_frames=max_frames,
                    )
                    if from_frame_no is None:
                        continue
                    found_to_frames.append(to_frame_no)
                    if sr.scan_mode == SubtitleRegion.ScanMode.SEARCH_SLICE:
                        yield {
                            "type": "subtitle_sign",
                            "from_frame_no": from_frame_no,
                            "to_frame_no": to_frame_no,
                            "initial_frame_no": frame_no,
                            "subtitle_text": subtitle_text,
                            "region": sr.name,
                            "position": [int(p) for p in subtitle_region],
                        }
                    else:
                        yield {
                            "type": "subtitle",
                            "from_frame_no": from_frame_no,
                            "to_frame_no": to_frame_no,
                            "initial_frame_no": frame_no,
                            "subtitle_text": subtitle_text,
                            "region": sr.name,
                        }
                else:
                    yield {
                        "type": "missed_region",
                        "frame_no": frame_no,
                        "region": sr.name,
                    }

            if found_to_frames:
                min_frame_no = min(found_to_frames)


def frame_generator(start_i):
    for i in range(1, 300):
        if i >= start_i:
            continue
        yield start_i + i
        yield start_i - i


def find_good_frame_breakpoint(video, current_frame):  # TODO: do binary search instead?
    compare_frame_size = (32, 32)
    frame_cache = {}

    def get_frame(frame_no):
        if frame_no not in frame_cache:
            video.set(cv2.CAP_PROP_POS_FRAMES, frame_no)
            frame_cache[frame_no] = cv2.cvtColor(
                cv2.resize(video.read()[1], compare_frame_size), cv2.COLOR_BGR2GRAY
            )
        return frame_cache[frame_no]

    best_score = 1.0
    best_frame = current_frame
    for frame_no in frame_generator(current_frame):
        score = structural_similarity(get_frame(frame_no), get_frame(frame_no + 1))
        if score < best_score:
            best_score = score
            best_frame = frame_no
        if score < 0.65:
            return frame_no - current_frame
    return best_frame - current_frame


def estimate_video_frame_diff(
    source_video, target_video, current_source_frame, current_target_frame
):
    frame_index_size = (64, 64)
    compare_frame_count = 5
    spread_frame_count = 14
    ret, source_frame = source_video.read()
    ret, target_frame = target_video.read()

    sy, sx, sz = source_frame.shape
    ty, tx, tz = target_frame.shape

    s_aspect = sx / sy
    t_aspect = tx / ty

    source_frames = []
    target_frames = []

    source_from_frame = current_source_frame - (compare_frame_count // 2)
    source_to_frame = source_from_frame + compare_frame_count

    source_video.set(cv2.CAP_PROP_POS_FRAMES, source_from_frame)
    for _ in range(source_from_frame, source_to_frame):
        frame_no = source_video.get(cv2.CAP_PROP_POS_FRAMES)
        source_frames.append(
            (
                cv2.cvtColor(
                    cv2.resize(source_video.read()[1], frame_index_size),
                    cv2.COLOR_BGR2GRAY,
                ),
                frame_no,
            )
        )

    target_from_frame = current_target_frame - spread_frame_count
    target_to_frame = target_from_frame + (spread_frame_count * 2)

    target_video.set(cv2.CAP_PROP_POS_FRAMES, target_from_frame)
    for _ in range(target_from_frame, target_to_frame):
        frame_no = target_video.get(cv2.CAP_PROP_POS_FRAMES)
        target_frame = target_video.read()[1]
        if s_aspect > t_aspect:
            new_tx = (tx / sx) * sy
            slice_each_x = int((tx - new_tx) / 2)
            target_frame = target_frame[0:ty, slice_each_x : (tx - slice_each_x)]

        target_frame = cv2.resize(target_frame, frame_index_size)
        target_frames.append((cv2.cvtColor(target_frame, cv2.COLOR_BGR2GRAY), frame_no))
    best_diff = 0
    best_frame_diff = None
    for i in range(len(target_frames) - len(source_frames)):
        v = target_frames[i : i + len(source_frames)]
        diffs = []
        frame_nos = []
        for sf, tf in zip(source_frames, v):
            sf, sfn = sf
            tf, tfn = tf
            frame_nos.append((sfn, tfn))
            diffs.append(structural_similarity(sf, tf, multichannel=False))
        diffs = np.square(np.array(diffs) * 100)
        if sum(diffs) > best_diff:
            best_diff = sum(diffs)
            best_frame_diff = (target_from_frame + i) - source_from_frame

    return best_frame_diff


def totimestamp(fps, frame_count):
    s = frame_count / fps
    m, s = divmod(s, 60)
    h, m = divmod(m, 60)
    return f"{int(h):02}:{int(m):02}:{(s):06.3f}"


def make_time(frames, fps, pos=None):
    ms = pysubs2.time.frames_to_ms(frames, fps)
    if pos == "end":
        ms = math.ceil((ms / 10)) * 10
    elif pos == "start":
        ms -= 10
    actual_frames = pysubs2.time.ms_to_frames(ms, fps)
    return ms


def save_frame_and_return_path(output_path, video, frame_no, position, region):
    fn = f"frame-{frame_no:05}"
    if position:
        fn += "-" + "-".join([str(i) for i in position])
    if region:
        fn += f"-r-{region.y}-{region.y + region.h}-{region.x}-{region.x + region.w}"
    fn += ".jpg"
    output_path.mkdir(exist_ok=True)
    output_file = output_path / fn
    if not output_file.exists():
        video.set(cv2.CAP_PROP_POS_FRAMES, frame_no)
        ret, frame = video.read()
        if position:
            cv2.rectangle(
                frame,
                (position[2], position[0]),
                (position[3], position[1]),
                (0, 255, 0),
                3,
            )
        if region:
            frame = frame[
                region.y : region.y + region.h, region.x : region.x + region.w
            ]
        if frame.shape[1] > 640:
            frame = cv2.resize(
                frame, (640, int(frame.shape[0] / (frame.shape[1] / 640)))
            )
        cv2.imwrite(str(output_file), frame)

    return f"{output_file.parent.name}/{output_file.name}"


def cleanup_text(text):
    text = text.replace("|", "I")
    return text


class Config:
    def __init__(self, temp_folder, video_path):
        self.lock = threading.Lock()
        self.path = temp_folder / f"{Path(video_path).name}.conf.json"
        if self.path.exists():
            self.config = json.loads(self.path.read_text())
        else:
            self.config = {}

    def get_frame_diff(self):
        with self.lock:
            return self.config.get("frame_diff")

    def set_frame_diff(self, frame_diff):
        with self.lock:
            self.config["frame_diff"] = frame_diff
            self._flush()

    def _flush(self):
        self.path.write_text(json.dumps(self.config, indent=2))

    def add_text_line(self, scan_mode, frame_range, progress, line=None):
        with self.lock:
            key = f"{scan_mode}-{frame_range[0]}-{frame_range[1]}"
            if "progress" not in self.config:
                self.config["progress"] = {}
            self.config["progress"][key] = progress
            if line:
                self.config.setdefault("lines", []).append(line)
            self._flush()

    def read_text_lines(self):
        with self.lock:
            return json.loads(self.path.read_text()).get("lines", [])


def pick_best_text(text_1, text_2):
    t_text_1 = TextBlob(text_1)
    t_text_2 = TextBlob(text_2)

    text_1_diff = normalized_damerau_levenshtein_distance(t_text_1, t_text_1.correct())
    text_2_diff = normalized_damerau_levenshtein_distance(t_text_2, t_text_2.correct())

    if text_2_diff < text_1_diff:
        return text_2
    else:
        return text_1


class FrameRangeParamType(click.ParamType):
    name = "framerange"

    def convert(self, value, param, ctx):
        value = value.split(":")
        if len(value) != 2:
            self.fail(
                "Missing arguments, syntax is start_frame:end_frame - can be negative"
            )

        try:
            start_frame = int(value[0])
            end_frame = int(value[1])
        except ValueError:
            self.fail(f"Value is wrong type, must be int")

        return start_frame, end_frame


FRAME_RANGE = FrameRangeParamType()


@click.group()
@click.argument("subtitled-file", type=click.Path(exists=True), required=True)
@click.argument("unsubtitled-file", type=click.Path(exists=True), required=True)
@click.option(
    "--temp-folder",
    type=click.Path(),
    default="cow-temp",
    help="Temp folder to store various files in.",
)
@click.option(
    "--subtitle-region-file",
    type=click.Path(),
    help="Subtitle region file.",
)
@click.pass_context
def cli(ctx, subtitled_file, unsubtitled_file, temp_folder, subtitle_region_file):
    temp_folder = Path(temp_folder)
    temp_folder.mkdir(exist_ok=True)

    ctx.ensure_object(dict)
    ctx.obj["subtitled_file"] = subtitled_file
    ctx.obj["unsubtitled_file"] = unsubtitled_file
    ctx.obj["temp_folder"] = temp_folder
    ctx.obj["config"] = Config(temp_folder, subtitled_file)
    if not subtitle_region_file:
        subtitle_region_file = temp_folder / "subtitle_regions.json"
    else:
        subtitle_region_file = Path(subtitle_region_file)

    if not subtitle_region_file.exists():
        subtitle_region_file.write_text(
            json.dumps(
                [dataclasses.asdict(sr) for sr in default_subtitle_regions], indent=2
            )
        )

    ctx.obj["subtitle_regions"] = [
        SubtitleRegion(**sr) for sr in json.loads(subtitle_region_file.read_text())
    ]


@cli.command()
@click.option(
    "--threads",
    type=int,
    default=3,
    help="Number of threads to parse video with.",
)
@click.option(
    "--tesseract-data-path",
    type=click.Path(exists=True),
    required=False,
)
@click.option(
    "--frame-diff",
    type=int,
    help="Set a frame diff manually",
)
@click.option(
    "--frame-range",
    type=FRAME_RANGE,
    help="Specify frame range to use.",
)
@click.option(
    "--ignore-diff-fps",
    is_flag=True,
    help="Ignore that FPS differ.",
)
@click.option(
    "--run-subregions-in-parallel",
    is_flag=True,
    help="Run all the subtitle regions in parallel instead of one at a time.",
)
@click.option(
    "--fix-broken-frame-alignment",
    is_flag=True,
    help="Sometimes one of the videos are broken frame-wise, try a few frames to see if we find a good match.",
)
@click.option(
    "--debug-frame",
    type=int,
    help="Debug a frame, output an image to show debug information.",
)
@click.option(
    "--debug-subregion",
    type=str,
    help="Choose a specific subtitle region, use in combination with debug-frame.",
)
@click.pass_context
def extract_subtitles(
    ctx,
    threads,
    tesseract_data_path,
    frame_diff,
    frame_range,
    ignore_diff_fps,
    run_subregions_in_parallel,
    fix_broken_frame_alignment,
    debug_frame,
    debug_subregion,
):  # subtitle_overwrite_region
    subtitle_regions = ctx.obj["subtitle_regions"]
    if tesseract_data_path is None:
        tesseract_data_path = (Path(__file__).parent / "tess-data").absolute()
    else:
        tesseract_data_path = Path(tesseract_data_path).absolute()
    tesseract_data_path = str(tesseract_data_path)

    config = ctx.obj["config"]
    src_video = cv2.VideoCapture(ctx.obj["subtitled_file"])
    dst_video = cv2.VideoCapture(ctx.obj["unsubtitled_file"])

    src_fps = fps = round(src_video.get(cv2.CAP_PROP_FPS), 3)
    dst_fps = round(dst_video.get(cv2.CAP_PROP_FPS), 3)
    if not ignore_diff_fps and src_fps != dst_fps:
        click.echo(
            f"Source and destination FPS are differnet {src_fps=} {dst_fps=} - this is not supported right now - use --ignore-diff-fps to ignore this"
        )
        quit(1)

    src_frame_count = src_video.get(cv2.CAP_PROP_FRAME_COUNT)
    dst_frame_count = src_video.get(cv2.CAP_PROP_FRAME_COUNT)
    max_frames = min(
        int(src_video.get(cv2.CAP_PROP_FRAME_COUNT)),
        int(dst_video.get(cv2.CAP_PROP_FRAME_COUNT)),
    )

    click.echo("Looking for video file frame difference")
    if frame_diff is None:
        frame_diff = config.get_frame_diff()
        if frame_diff is None:  # TODO: use DTW to find big diffs
            click.echo("No frame diff found in cache, detecting...")
            source_frame = int(max_frames * 0.6)
            target_frame = source_frame

            good_breakpoint_diff = find_good_frame_breakpoint(src_video, source_frame)
            frame_diff = estimate_video_frame_diff(
                src_video,
                dst_video,
                source_frame + good_breakpoint_diff,
                target_frame + good_breakpoint_diff,
            )
            config.set_frame_diff(frame_diff)
    click.echo(f"Using frame diff {frame_diff}")

    if debug_frame is not None:
        debug_folder = ctx.obj["temp_folder"] / "debug"
        debug_folder.mkdir(exist_ok=True)
        start_frame_no = debug_frame
        end_frame_no = start_frame_no + 1
        if frame_diff < 0:
            end_frame_no -= frame_diff

        img = get_frame(src_video, start_frame_no)
        for sr in subtitle_regions:
            cv2.rectangle(img, (sr.x, sr.y), (sr.x + sr.w, sr.y + sr.h), (0, 255, 0), 2)
            cv2.putText(
                img,
                sr.name,
                (20, sr.y + sr.h - 20),
                cv2.FONT_HERSHEY_SIMPLEX,
                2,
                (0, 255, 0),
                2,
                cv2.LINE_AA,
            )
        fn = debug_folder / f"subtitle_regions.png"
        cv2.imwrite(str(fn.absolute()), img)

        if debug_subregion is not None:
            sr = [sr for sr in subtitle_regions if sr.name == debug_subregion][0]
        else:
            sr = subtitle_regions[0]

        gather_images = []
        for result in loop_frames(
            src_video,
            dst_video,
            start_frame_no,
            end_frame_no,
            frame_diff,
            sr,
            tesseract_data_path,
            fix_broken_frame_alignment=fix_broken_frame_alignment,
            gather_images=gather_images,
            edge_threshold=sr.edge_threshold,
            max_frames=max_frames,
        ):
            for i, (title, img) in enumerate(gather_images):
                fn = debug_folder / f"{i:02}.png"
                print(title, str(fn))
                if img is None or not img.shape:
                    print(f"Skipping image {title}")
                    continue
                cv2.imwrite(str(fn.absolute()), img)
            print(result)
            break
    else:
        click.echo("Starting to extracting actual subtitles")

        def ocr_in_thread(start_frame_no, end_frame_no, frame_diff, sr):
            src_video = cv2.VideoCapture(ctx.obj["subtitled_file"])
            dst_video = cv2.VideoCapture(ctx.obj["unsubtitled_file"])
            for result in loop_frames(
                src_video,
                dst_video,
                start_frame_no,
                end_frame_no,
                frame_diff,
                sr,
                tesseract_data_path,
                fix_broken_frame_alignment=fix_broken_frame_alignment,
                gather_images=None,
                edge_threshold=sr.edge_threshold,
                max_frames=max_frames,
            ):
                config.add_text_line(
                    sr.scan_mode,
                    (start_frame_no, end_frame_no),
                    result.get("initial_frame_no", result.get("frame_no", 0)),
                    result,
                )

        max_workers = threads
        if run_subregions_in_parallel:
            max_workers *= len(subtitle_regions)
        with concurrent.futures.ThreadPoolExecutor(max_workers=max_workers) as executor:
            if frame_range is None:
                frame_range = (0, max_frames)

            if frame_range[1] < 0:
                frame_range = (frame_range[0], max_frames - frame_range[1])

            frames_per_thread = math.ceil((frame_range[1] - frame_range[0]) / threads)
            jobs = {}
            for sr in subtitle_regions:
                for i in range(threads):
                    start_frame_no = frame_range[0] + (frames_per_thread * i)
                    end_frame_no = frame_range[0] + (
                        min(frames_per_thread * (i + 1), max_frames)
                    )
                    jobs[
                        executor.submit(
                            ocr_in_thread, start_frame_no, end_frame_no, frame_diff, sr
                        )
                    ] = (sr, start_frame_no, end_frame_no)

            for future in concurrent.futures.as_completed(jobs):
                sr, start_frame_no, end_frame_no = jobs[future]
                try:
                    future.result()
                except Exception as exc:
                    click.echo(
                        f"Failed job {sr.name} {start_frame_no=} {end_frame_no=}"
                    )
                    traceback.print_exc()
                # else:
                #     click.echo(f"Done with job {sr.name} {start_frame_no=} {end_frame_no=}")
    click.echo(f"Done generating subtitle json from: {str(ctx.obj['subtitled_file'])}")


@cli.command()
@click.option(
    "--output-report-path",
    type=click.Path(exists=True),
    required=False,
    help="Folder to save reports to",
)
@click.option(
    "--output-subtitle-path",
    type=click.Path(exists=True),
    required=False,
    help="Folder to save the subtitles to",
)
@click.pass_context
def create_report(ctx, output_report_path, output_subtitle_path):
    src_video = cv2.VideoCapture(ctx.obj["subtitled_file"])
    max_frames = src_video.get(cv2.CAP_PROP_FRAME_COUNT) - 1
    fps = src_video.get(cv2.CAP_PROP_FPS)
    env = jinja2.Environment()
    env.filters["totimestamp"] = lambda frame_count: totimestamp(fps, frame_count)
    if output_report_path is None:
        output_report_path = (
            ctx.obj["temp_folder"] / f"{Path(ctx.obj['subtitled_file']).name}-report"
        )
    else:
        output_report_path = Path(output_report_path)

    output_report_path.mkdir(exist_ok=True)

    if output_subtitle_path is None:
        output_subtitle_path = (
            ctx.obj["temp_folder"]
            / f"{Path(ctx.obj['subtitled_file']).with_suffix('.ass').name}"
        )
    else:
        output_subtitle_path = Path(output_subtitle_path)

    config = ctx.obj["config"]

    def _get_frame(frame_no, position=None, region=None):
        frame_no = min(max_frames, frame_no)
        return save_frame_and_return_path(
            output_report_path / "images", src_video, frame_no, position, region
        )

    env.globals["get_frame"] = _get_frame

    subs = pysubs2.SSAFile.from_string(BASE_ASS)
    subs.info["PlayResX"] = int(src_video.get(cv2.CAP_PROP_FRAME_WIDTH))
    subs.info["PlayResY"] = int(src_video.get(cv2.CAP_PROP_FRAME_HEIGHT))

    subtitle_regions = ctx.obj["subtitle_regions"]

    subtitle_region_data = {}
    for sr in subtitle_regions:
        subtitle_region_data[sr.name] = {
            "subtitle_lines": [],
            "subtitle_signs": [],
            "missing_regions": [],
        }
        missing_regions, subtitles, subtitle_signs = (
            subtitle_region_data[sr.name]["missing_regions"],
            subtitle_region_data[sr.name]["subtitle_lines"],
            subtitle_region_data[sr.name]["subtitle_signs"],
        )
        for l in config.read_text_lines():
            if l["region"] != sr.name:
                continue
            if "to_frame_no" in l:
                l["to_frame_no"] = min(l["to_frame_no"], max_frames)
                l["from_frame_no"] = min(l["to_frame_no"], l["from_frame_no"])
            if l["type"] == "subtitle":
                subtitles.append(l)
            elif l["type"] == "subtitle_sign":
                subtitle_signs.append(l)
            elif l["type"] == "missed_region":
                missing_regions.append(l)
            else:
                click.echo(f"unknown type {l['type']}")

        missing_regions = sorted(missing_regions, key=lambda x: x["frame_no"])
        subtitles = sorted(subtitles, key=lambda x: x["from_frame_no"])
        subtitle_signs = sorted(subtitle_signs, key=lambda x: x["from_frame_no"])

        missing_frames = set()
        for missing_region in missing_regions:
            missing_frames.add(missing_region["frame_no"])

        cleaned_subtitles = []
        last_line = None
        for line in subtitles:
            line["subtitle_text"] = cleanup_text(line["subtitle_text"])
            if last_line is not None:
                if (
                    last_line["to_frame_no"] > line["from_frame_no"] - 3
                    and normalized_damerau_levenshtein_distance(
                        last_line["subtitle_text"], line["subtitle_text"]
                    )
                    < 0.2
                ):
                    # TODO: make sure all intersecting lines are removed too
                    last_line["from_frame_no"] = min(
                        last_line["from_frame_no"], line["from_frame_no"]
                    )
                    last_line["to_frame_no"] = max(
                        last_line["to_frame_no"], line["to_frame_no"]
                    )
                    last_line["subtitle_text"] = pick_best_text(
                        last_line["subtitle_text"], line["subtitle_text"]
                    )
                    click.echo(f"Merging: {line} {last_line}")
                    continue
            for frame_no in range(line["from_frame_no"], line["to_frame_no"] + 1):
                if frame_no in missing_frames:
                    missing_frames.remove(frame_no)
                    click.echo(f"Frame {frame_no} found in missing frames")
            cleaned_subtitles.append(line)
            last_line = line

        if sr.scan_mode == SubtitleRegion.ScanMode.BOTTOM_CENTER:
            short_subtitle_signs = [
                line
                for line in cleaned_subtitles
                if line["to_frame_no"] - line["from_frame_no"] <= 4
            ]
            cleaned_subtitles = [
                line
                for line in cleaned_subtitles
                if line["to_frame_no"] - line["from_frame_no"] > 4
            ]
            (output_report_path / f"{sr.name}-subtitles.html").write_text(
                env.from_string(HTML_SUBTITLE_LINES).render(
                    subtitle_lines=cleaned_subtitles, sr=sr, title="CowOCR - Subtitles"
                )
            )

            for line in cleaned_subtitles:
                subtitle_text = line["subtitle_text"].replace("\n", "\\N")
                subs.append(
                    pysubs2.SSAEvent(
                        start=make_time(
                            frames=line["from_frame_no"], fps=fps, pos="start"
                        ),
                        end=make_time(frames=line["to_frame_no"], fps=fps, pos="end"),
                        text=subtitle_text,
                        style=sr.ass_style_name,
                    )
                )

        if sr.scan_mode == SubtitleRegion.ScanMode.SEARCH_SLICE:
            short_subtitle_signs = [
                subtitle_sign
                for subtitle_sign in subtitle_signs
                if subtitle_sign["to_frame_no"] - subtitle_sign["from_frame_no"] <= 10
            ]
        missing_frames = [
            missing_region
            for missing_region in missing_regions
            if missing_region["frame_no"] in missing_frames
        ]
        (output_report_path / f"{sr.name}-missing-regions.html").write_text(
            env.from_string(HTML_MISSING_REGIONS).render(
                missing_regions=missing_frames,
                sr=sr,
                short_subtitle_signs=short_subtitle_signs,
                title="CowOCR - Missing regions",
            )
        )

        if sr.scan_mode == SubtitleRegion.ScanMode.SEARCH_SLICE:
            subtitle_signs = [
                subtitle_sign
                for subtitle_sign in subtitle_signs
                if subtitle_sign["to_frame_no"] - subtitle_sign["from_frame_no"] > 10
            ]
            for subtitle_sign in subtitle_signs:
                subtitle_sign["subtitle_text"] = cleanup_text(
                    subtitle_sign["subtitle_text"]
                )
            (output_report_path / f"{sr.name}-subtitle-signs.html").write_text(
                env.from_string(HTML_SUBTITLE_SIGNS).render(
                    subtitle_signs=subtitle_signs,
                    sr=sr,
                    title="CowOCR - Subtitle Signs",
                )
            )

            for subtitle_sign in subtitle_signs:
                subtitle_text = subtitle_sign["subtitle_text"].replace("\n", "\\N")
                subtitle_text = (
                    "{\pos(%s,%s)}"
                    % (
                        int(
                            (
                                subtitle_sign["position"][2]
                                + subtitle_sign["position"][3]
                            )
                            / 2
                        ),
                        subtitle_sign["position"][1],
                    )
                ) + subtitle_text
                subs.append(
                    pysubs2.SSAEvent(
                        start=make_time(
                            frames=subtitle_sign["from_frame_no"], fps=fps, pos="start"
                        ),
                        end=make_time(
                            frames=subtitle_sign["to_frame_no"], fps=fps, pos="end"
                        ),
                        text=subtitle_text,
                        style=sr.ass_style_name,
                    )
                )

    (output_report_path / "index.html").write_text(
        env.from_string(HTML_INDEX).render(
            subtitle_regions=subtitle_regions,
            subtitle_region_data=subtitle_region_data,
            title="CowOCR - Index",
        )
    )

    subs.save(str(output_subtitle_path.absolute()))

    click.echo(f"Saved report to: {str(output_report_path)}")
    click.echo(f"Saved subtitles to: {str(output_subtitle_path)}")


if __name__ == "__main__":
    cli()
